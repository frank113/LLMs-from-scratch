{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48154d70",
   "metadata": {},
   "source": [
    "# Weight Loading Deep Dive\n",
    "\n",
    "This notebook will make sense of the `load_weights` function introduced in chapter 5 and referenced in `weight-loading-hf-transformers` supplementary material. We will first load all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0826239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fkovacs/Documents/GitHub/ai/LLMs-from-scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "torch version: 2.5.1\n",
      "transformers version: 4.57.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import GPT2Model\n",
    "\n",
    "pkgs = [\"numpy\", \"torch\", \"transformers\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3707f",
   "metadata": {},
   "source": [
    "Create model configs. Note that we have externalized them into the `model_config` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70391c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"model_config/base_config.json\", \"r\") as base_file:\n",
    "\tBASE_CONFIG = json.load(base_file)\n",
    "\n",
    "with open(\"model_config/model_configs.json\", \"r\") as model_file:\n",
    "\tmodel_configs = json.load(model_file)\n",
    "\n",
    "with open(\"model_config/model_names.json\", \"r\") as names_file:\n",
    "\tmodel_names = json.load(names_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a635a",
   "metadata": {},
   "source": [
    "We note that we will be working with the model `gpt2-small (124M)`. We will therefore use the `CHOOSE_MODEL` variable to index into the appropriate configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49da421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59c282",
   "metadata": {},
   "source": [
    "We will now instantiate references to the model in the `gpt_hf` variable. Note the different `checkpoint` directory to ensure we do not conflict with the primary notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076b4de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_checkpoint_dir = \"checkpoint_deep_dive\"\n",
    "gpt_hf = GPT2Model.from_pretrained(\n",
    "\tmodel_names[CHOOSE_MODEL], \n",
    "\tcache_dir=alt_checkpoint_dir\n",
    ")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89664d0",
   "metadata": {},
   "source": [
    "We will now utilize `update` to provide extra fields to `BASE_CONFIG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896627c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "print(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f56358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580e699",
   "metadata": {},
   "source": [
    "We now reach the ugly function. It is reproduced below for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c536e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6e5cc",
   "metadata": {},
   "source": [
    "We first capture the state dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07f3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
